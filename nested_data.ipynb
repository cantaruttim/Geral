{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGjh+zlW1GGJCi43Kdn1un"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FdOwe-Z7dzMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ac92c6-be95-4f8d-a84e-1590fb3f3a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, from_json, expr, explode, arrays_zip, lit\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "\n",
        "# Iniciar a sessão\n",
        "spark = SparkSession.builder.appName(\"DynamicColumnExtraction\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    ('[\"nome\", \"idade\", \"ativo\"]', '[\"João\", \"30\", \"true\"]'),\n",
        "    ('[\"nome\", \"idade\", \"ativo\"]', '[\"Maria\", \"25\", \"false\"]')\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"metadata\", \"valores\"])\n",
        "\n",
        "df_parsed = (\n",
        "    df.withColumn(\"metadata_array\", from_json(col(\"metadata\"), ArrayType(StringType())))\n",
        "      .withColumn(\"valores_array\", from_json(col(\"valores\"), ArrayType(StringType())))\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "df_zipped = df_parsed.withColumn(\"zipped\", arrays_zip(\"metadata_array\", \"valores_array\"))\n",
        "\n",
        "\n",
        "\n",
        "df_exploded = df_zipped.withColumn(\"kv\", explode(col(\"zipped\")))\n",
        "\n",
        "df_mapped = (\n",
        "    df_exploded.withColumn(\"key\", col(\"kv\")[\"metadata_array\"])\n",
        "                       .withColumn(\"value\", col(\"kv\")[\"valores_array\"])\n",
        "                       .groupBy(\"metadata\", \"valores\")\n",
        "                       .pivot(\"key\").agg(expr(\"first(value)\"))\n",
        ")\n",
        "\n",
        "df_mapped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryAcwkYRi9UI",
        "outputId": "7d52a51b-ef78-495b-f3f1-9de43a1061ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----+-----+-----+\n",
            "|            metadata|             valores|ativo|idade| nome|\n",
            "+--------------------+--------------------+-----+-----+-----+\n",
            "|[\"nome\", \"idade\",...|[\"João\", \"30\", \"t...| true|   30| João|\n",
            "|[\"nome\", \"idade\",...|[\"Maria\", \"25\", \"...|false|   25|Maria|\n",
            "+--------------------+--------------------+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType, BooleanType\n",
        "\n",
        "def infer_and_cast(df, columns):\n",
        "    for col_name in columns:\n",
        "        sample_value = df.select(col_name).filter(col(col_name).isNotNull()).first()\n",
        "        if sample_value:\n",
        "            value = sample_value[0]\n",
        "            if value.lower() in (\"true\", \"false\"):\n",
        "                df = df.withColumn(col_name, col(col_name).cast(BooleanType()))\n",
        "            elif value.isdigit():\n",
        "                df = df.withColumn(col_name, col(col_name).cast(IntegerType()))\n",
        "    return df\n",
        "\n",
        "# Aplicar inferência\n",
        "columns_to_infer = [col for col in df_mapped.columns if col not in (\"metadata\", \"valores\")]\n",
        "df_final = infer_and_cast(df_mapped, columns_to_infer)\n",
        "\n",
        "df_final.show()\n",
        "df_final.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r5kT_RSkCdM",
        "outputId": "90dcb764-5e3e-4487-95ec-a70267c8e3fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----+-----+-----+\n",
            "|            metadata|             valores|ativo|idade| nome|\n",
            "+--------------------+--------------------+-----+-----+-----+\n",
            "|[\"nome\", \"idade\",...|[\"João\", \"30\", \"t...| true|   30| João|\n",
            "|[\"nome\", \"idade\",...|[\"Maria\", \"25\", \"...|false|   25|Maria|\n",
            "+--------------------+--------------------+-----+-----+-----+\n",
            "\n",
            "root\n",
            " |-- metadata: string (nullable = true)\n",
            " |-- valores: string (nullable = true)\n",
            " |-- ativo: boolean (nullable = true)\n",
            " |-- idade: integer (nullable = true)\n",
            " |-- nome: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_objects = [\n",
        "    {\n",
        "        \"metadata\": [\"pessoa.nome\", \"pessoa.idade\", \"pessoa.ativo\", \"pessoa.salario\", \"empresa.nome\"],\n",
        "        \"valores\": [\"João\", \"30\", \"true\", \"1500.75\", \"ACME Ltda\"]\n",
        "    },\n",
        "    {\n",
        "        \"metadata\": [\"pessoa.nome\", \"pessoa.idade\", \"pessoa.ativo\", \"pessoa.salario\", \"empresa.nome\"],\n",
        "        \"valores\": [\"Maria\", \"25\", \"false\", \"2000.00\", \"Beta Corp\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "raw_objects"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtLItovMoafQ",
        "outputId": "5b66896e-6ad1-415e-f897-70b056859571"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'metadata': ['pessoa.nome',\n",
              "   'pessoa.idade',\n",
              "   'pessoa.ativo',\n",
              "   'pessoa.salario',\n",
              "   'empresa.nome'],\n",
              "  'valores': ['João', '30', 'true', '1500.75', 'ACME Ltda']},\n",
              " {'metadata': ['pessoa.nome',\n",
              "   'pessoa.idade',\n",
              "   'pessoa.ativo',\n",
              "   'pessoa.salario',\n",
              "   'empresa.nome'],\n",
              "  'valores': ['Maria', '25', 'false', '2000.00', 'Beta Corp']}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Versão Um"
      ],
      "metadata": {
        "id": "HX-322bxpCA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 1. SETUP\n",
        "!pip install -q pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"NestedStruct\").getOrCreate()\n",
        "\n",
        "# ✅ 2. DADOS DE ENTRADA (JSON SERIALIZADO EM STRING)\n",
        "raw_objects = [\n",
        "    {\n",
        "        \"metadata\": [\"pessoa.nome\", \"pessoa.idade\", \"pessoa.ativo\", \"pessoa.salario\", \"empresa.nome\"],\n",
        "        \"valores\": [\"João\", \"30\", \"true\", \"1500.75\", \"ACME Ltda\"]\n",
        "    },\n",
        "    {\n",
        "        \"metadata\": [\"pessoa.nome\", \"pessoa.idade\", \"pessoa.ativo\", \"pessoa.salario\", \"empresa.nome\"],\n",
        "        \"valores\": [\"Maria\", \"25\", \"false\", \"2000.00\", \"Beta Corp\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Serializar como strings JSON\n",
        "data = [(json.dumps(obj),) for obj in raw_objects]\n",
        "\n",
        "# Criar DataFrame com coluna única 'linha'\n",
        "schema = StructType([StructField(\"linha\", StringType(), True)])\n",
        "df_raw = spark.createDataFrame(data, schema)\n",
        "\n",
        "# ✅ 3. PARSEAR JSON\n",
        "json_schema = StructType([\n",
        "    StructField(\"metadata\", ArrayType(StringType())),\n",
        "    StructField(\"valores\", ArrayType(StringType()))\n",
        "])\n",
        "\n",
        "df = df_raw.withColumn(\"parsed\", from_json(\"linha\", json_schema)) \\\n",
        "    .selectExpr(\"parsed.metadata\", \"parsed.valores\") \\\n",
        "    .withColumn(\"zipped\", arrays_zip(\"metadata\", \"valores\")) \\\n",
        "    .withColumn(\"pair\", explode(\"zipped\")) \\\n",
        "    .withColumn(\"chave\", col(\"pair.metadata\")) \\\n",
        "    .withColumn(\"valor\", col(\"pair.valores\"))\n",
        "\n",
        "# ✅ 4. ADICIONAR ID POR LINHA PARA RECONSTRUIR DEPOIS\n",
        "df = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
        "\n",
        "# ✅ 5. PIVOTAR: CHAVE COMO COLUNA (COM SUBSTITUIÇÃO DE . POR _)\n",
        "df_pivoted = df.groupBy(\"row_id\").pivot(\"chave\").agg(first(\"valor\"))\n",
        "\n",
        "# Renomear colunas substituindo . por _\n",
        "for old_name in df_pivoted.columns:\n",
        "    if old_name != \"row_id\":\n",
        "        new_name = old_name.replace('.', '_')\n",
        "        df_pivoted = df_pivoted.withColumnRenamed(old_name, new_name)\n",
        "\n",
        "# ✅ 6. INFERÊNCIA DE TIPO DINÂMICA\n",
        "def infer_and_cast(df):\n",
        "    for c in df.columns:\n",
        "        if c == \"row_id\":\n",
        "            continue\n",
        "\n",
        "        samples = df.select(c).filter(col(c).isNotNull()).limit(10).collect()\n",
        "        if not samples:\n",
        "            continue\n",
        "\n",
        "        types = set()\n",
        "        for row in samples:\n",
        "            value = row[0]\n",
        "            if isinstance(value, str):\n",
        "                if value.lower() in (\"true\", \"false\"):\n",
        "                    types.add(\"boolean\")\n",
        "                elif re.match(r\"^-?\\d+$\", value):\n",
        "                    types.add(\"int\")\n",
        "                elif re.match(r\"^-?\\d+\\.\\d+$\", value):\n",
        "                    types.add(\"float\")\n",
        "                else:\n",
        "                    types.add(\"string\")\n",
        "            else:\n",
        "                types.add(type(value).__name__)\n",
        "\n",
        "        if len(types) == 1:\n",
        "            inferred = types.pop()\n",
        "            if inferred == \"boolean\":\n",
        "                df = df.withColumn(c, col(c).cast(BooleanType()))\n",
        "            elif inferred == \"int\":\n",
        "                df = df.withColumn(c, col(c).cast(IntegerType()))\n",
        "            elif inferred == \"float\":\n",
        "                df = df.withColumn(c, col(c).cast(FloatType()))\n",
        "            else:\n",
        "                df = df.withColumn(c, col(c).cast(StringType()))\n",
        "        else:\n",
        "            df = df.withColumn(c, col(c).cast(StringType()))\n",
        "    return df\n",
        "\n",
        "df_casted = infer_and_cast(df_pivoted)\n",
        "\n",
        "# ✅ 7. RECONSTRUIR STRUCTS COM BASE EM PREFIXOS (AGORA USANDO _ EM VEZ DE .)\n",
        "def group_by_prefix(columns):\n",
        "    grouped = defaultdict(list)\n",
        "    for c in columns:\n",
        "        if '_' in c and c != \"row_id\":\n",
        "            prefix, field = c.split('_', 1)\n",
        "            grouped[prefix].append((c, field))\n",
        "    return grouped\n",
        "\n",
        "grouped = group_by_prefix(df_casted.columns)\n",
        "\n",
        "for prefix, fields in grouped.items():\n",
        "    struct_cols = [col(full).alias(sub) for full, sub in fields]\n",
        "    df_casted = df_casted.withColumn(prefix, struct(*struct_cols))\n",
        "\n",
        "# ✅ 8. REMOVER COLUNAS FLAT COM UNDERSCORES\n",
        "flat_cols = [c for c in df_casted.columns if '_' not in c and c not in grouped]\n",
        "final_cols = flat_cols + list(grouped.keys())\n",
        "df_final = df_casted.select(final_cols)\n",
        "\n",
        "# ✅ 9. RESULTADO\n",
        "df_final.printSchema()\n",
        "df_final.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0pCRcLNnVwD",
        "outputId": "a8463c20-0f6e-41b9-9a17-07cc8254ccb1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- empresa: struct (nullable = false)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |-- pessoa: struct (nullable = false)\n",
            " |    |-- ativo: boolean (nullable = true)\n",
            " |    |-- idade: integer (nullable = true)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |    |-- salario: float (nullable = true)\n",
            "\n",
            "+-----------+---------------------------+\n",
            "|empresa    |pessoa                     |\n",
            "+-----------+---------------------------+\n",
            "|{NULL}     |{NULL, NULL, João, NULL}   |\n",
            "|{NULL}     |{NULL, 30, NULL, NULL}     |\n",
            "|{NULL}     |{true, NULL, NULL, NULL}   |\n",
            "|{NULL}     |{NULL, NULL, NULL, 1500.75}|\n",
            "|{ACME Ltda}|{NULL, NULL, NULL, NULL}   |\n",
            "|{NULL}     |{NULL, NULL, Maria, NULL}  |\n",
            "|{NULL}     |{NULL, 25, NULL, NULL}     |\n",
            "|{NULL}     |{false, NULL, NULL, NULL}  |\n",
            "|{NULL}     |{NULL, NULL, NULL, 2000.0} |\n",
            "|{Beta Corp}|{NULL, NULL, NULL, NULL}   |\n",
            "+-----------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Versão Dois"
      ],
      "metadata": {
        "id": "ah-ApJBUpERb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Dados de entrada\n",
        "raw_objects = [\n",
        "    {\n",
        "        \"metadata\": [\"pessoa.nome\", \"pessoa.idade\", \"pessoa.ativo\", \"pessoa.salario\", \"empresa.nome\", \"VL_LIST_DATA\"],\n",
        "        \"valores\": [\"João\", \"30\", \"true\", \"1500.75\", \"ACME Ltda\", \"detalhes.cargo=Analista;detalhes.setor=TI;contatos.0.tipo=email;contatos.0.valor=joao@email.com\"]\n",
        "    },\n",
        "    {\n",
        "        \"metadata\": [\"pessoa.nome\", \"pessoa.idade\", \"pessoa.ativo\", \"pessoa.salario\", \"empresa.nome\", \"VL_LIST_DATA\"],\n",
        "        \"valores\": [\"Maria\", \"25\", \"false\", \"2000.00\", \"Beta Corp\", \"detalhes.cargo=Gerente;detalhes.setor=RH;contatos.0.tipo=telefone;contatos.0.valor=1199999999\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# 1. Criar DataFrame inicial\n",
        "data = [(json.dumps(obj),) for obj in raw_objects]\n",
        "df_raw = spark.createDataFrame(data, [\"linha\"])\n",
        "\n",
        "# 2. Parsear JSON\n",
        "json_schema = StructType([\n",
        "    StructField(\"metadata\", ArrayType(StringType())),\n",
        "    StructField(\"valores\", ArrayType(StringType()))\n",
        "])\n",
        "\n",
        "df = df_raw.withColumn(\"parsed\", F.from_json(\"linha\", json_schema)) \\\n",
        "           .select(\"parsed.*\") \\\n",
        "           .withColumn(\"row_id\", F.monotonically_increasing_id())\n",
        "\n",
        "# 3. Explodir para pares chave-valor\n",
        "df_exploded = df.withColumn(\"zipped\", F.arrays_zip(\"metadata\", \"valores\")) \\\n",
        "                .withColumn(\"pair\", F.explode(\"zipped\")) \\\n",
        "                .select(\n",
        "                    \"row_id\",\n",
        "                    F.col(\"pair.metadata\").alias(\"chave\"),\n",
        "                    F.col(\"pair.valores\").alias(\"valor\")\n",
        "                )\n",
        "\n",
        "# 4. Separar dados normais e VL_LIST_DATA\n",
        "df_main = df_exploded.filter(F.col(\"chave\") != \"VL_LIST_DATA\")\n",
        "df_list_data = df_exploded.filter(F.col(\"chave\") == \"VL_LIST_DATA\")\n",
        "\n",
        "# 5. Pivotar dados principais (substituindo . por _)\n",
        "df_pivoted = df_main.withColumn(\"chave\", F.regexp_replace(\"chave\", \"\\\\.\", \"_\")) \\\n",
        "                   .groupBy(\"row_id\").pivot(\"chave\").agg(F.first(\"valor\"))\n",
        "\n",
        "# 6. Processar VL_LIST_DATA\n",
        "def parse_list_data(value):\n",
        "    result = {\"detalhes\": {}, \"contatos\": []}\n",
        "    items = value.split(';')\n",
        "    for item in items:\n",
        "        if '=' in item:\n",
        "            key, val = item.split('=', 1)\n",
        "            parts = key.split('.')\n",
        "            if parts[0] == \"detalhes\":\n",
        "                result[\"detalhes\"][parts[1]] = val\n",
        "            elif parts[0] == \"contatos\":\n",
        "                idx = int(parts[1])\n",
        "                while len(result[\"contatos\"]) <= idx:\n",
        "                    result[\"contatos\"].append({})\n",
        "                result[\"contatos\"][idx][parts[2]] = val\n",
        "    return json.dumps(result)\n",
        "\n",
        "parse_list_data_udf = F.udf(parse_list_data, StringType())\n",
        "\n",
        "df_list_data_processed = df_list_data.withColumn(\n",
        "    \"parsed_list_data\",\n",
        "    parse_list_data_udf(\"valor\")\n",
        ").select(\"row_id\", \"parsed_list_data\")\n",
        "\n",
        "# 7. Juntar tudo\n",
        "df_combined = df_pivoted.join(df_list_data_processed, \"row_id\")\n",
        "\n",
        "# 8. Inferir tipos\n",
        "def infer_type(value):\n",
        "    if value.lower() in (\"true\", \"false\"):\n",
        "        return BooleanType()\n",
        "    elif re.match(r\"^-?\\d+$\", value):\n",
        "        return IntegerType()\n",
        "    elif re.match(r\"^-?\\d+\\.\\d+$\", value):\n",
        "        return FloatType()\n",
        "    return StringType()\n",
        "\n",
        "for col_name in df_combined.columns:\n",
        "    if col_name not in [\"row_id\", \"parsed_list_data\"]:\n",
        "        sample = df_combined.select(col_name).filter(F.col(col_name).isNotNull()).first()\n",
        "        if sample:\n",
        "            dtype = infer_type(sample[0])\n",
        "            df_combined = df_combined.withColumn(col_name, F.col(col_name).cast(dtype))\n",
        "\n",
        "# 9. Criar estruturas aninhadas\n",
        "def create_struct(df, prefix):\n",
        "    cols = [c for c in df.columns if c.startswith(prefix + \"_\")]\n",
        "    if not cols:\n",
        "        return df\n",
        "\n",
        "    fields = [c[len(prefix)+1:] for c in cols]\n",
        "    struct_cols = [F.col(c).alias(f) for c, f in zip(cols, fields)]\n",
        "\n",
        "    return df.withColumn(prefix, F.struct(*struct_cols)).drop(*cols)\n",
        "\n",
        "df_final = create_struct(df_combined, \"pessoa\")\n",
        "df_final = create_struct(df_final, \"empresa\")\n",
        "\n",
        "# 10. Processar VL_LIST_DATA\n",
        "list_data_schema = StructType([\n",
        "    StructField(\"detalhes\", StructType([\n",
        "        StructField(\"cargo\", StringType()),\n",
        "        StructField(\"setor\", StringType())\n",
        "    ])),\n",
        "    StructField(\"contatos\", ArrayType(StructType([\n",
        "        StructField(\"tipo\", StringType()),\n",
        "        StructField(\"valor\", StringType())\n",
        "    ])))\n",
        "])\n",
        "\n",
        "df_final = df_final.withColumn(\n",
        "    \"VL_LIST_DATA\",\n",
        "    F.from_json(\"parsed_list_data\", list_data_schema)\n",
        ").drop(\"parsed_list_data\")\n",
        "\n",
        "# Resultado final\n",
        "df_final.printSchema()\n",
        "df_final.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jxInpwKoQ4G",
        "outputId": "6dd41893-016a-41b0-faf3-d16f13dc662e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- row_id: long (nullable = false)\n",
            " |-- pessoa: struct (nullable = false)\n",
            " |    |-- ativo: boolean (nullable = true)\n",
            " |    |-- idade: integer (nullable = true)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |    |-- salario: float (nullable = true)\n",
            " |-- empresa: struct (nullable = false)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |-- VL_LIST_DATA: struct (nullable = true)\n",
            " |    |-- detalhes: struct (nullable = true)\n",
            " |    |    |-- cargo: string (nullable = true)\n",
            " |    |    |-- setor: string (nullable = true)\n",
            " |    |-- contatos: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- tipo: string (nullable = true)\n",
            " |    |    |    |-- valor: string (nullable = true)\n",
            "\n",
            "+----------+--------------------------+-----------+-------------------------------------------+\n",
            "|row_id    |pessoa                    |empresa    |VL_LIST_DATA                               |\n",
            "+----------+--------------------------+-----------+-------------------------------------------+\n",
            "|0         |{true, 30, João, 1500.75} |{ACME Ltda}|{{Analista, TI}, [{email, joao@email.com}]}|\n",
            "|8589934592|{false, 25, Maria, 2000.0}|{Beta Corp}|{{Gerente, RH}, [{telefone, 1199999999}]}  |\n",
            "+----------+--------------------------+-----------+-------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "\n",
        "# Função para extrair schema dinâmico de VL_LIST_API\n",
        "def get_dynamic_schema(df):\n",
        "    # Coletar amostras para identificar todos os campos possíveis\n",
        "    samples = df.select(\"VL_LIST_DATA\").limit(100).collect()\n",
        "\n",
        "    # Identificar todos os caminhos de campos\n",
        "    field_paths = set()\n",
        "\n",
        "    for row in samples:\n",
        "        data = row.VL_LIST_DATA\n",
        "        if data:\n",
        "            # Extrair campos dos detalhes\n",
        "            if data.detalhes:\n",
        "                for field in data.detalhes.__fields__:\n",
        "                    field_paths.add(f\"detalhes_{field}\")\n",
        "\n",
        "            # Extrair campos dos contatos\n",
        "            if data.contatos:\n",
        "                for contact in data.contatos:\n",
        "                    for field in contact.__fields__:\n",
        "                        field_paths.add(f\"contatos_{field}\")\n",
        "\n",
        "    return sorted(field_paths)\n",
        "\n",
        "# Função para extrair valores dinamicamente\n",
        "def extract_dynamic_fields(df):\n",
        "    # Obter schema dinâmico\n",
        "    dynamic_fields = get_dynamic_schema(df)\n",
        "\n",
        "    # Adicionar colunas para cada campo dinâmico\n",
        "    for field in dynamic_fields:\n",
        "        parts = field.split('_')\n",
        "        if parts[0] == \"detalhes\":\n",
        "            df = df.withColumn(field, F.col(\"VL_LIST_DATA.detalhes\").getItem(parts[1]))\n",
        "        elif parts[0] == \"contatos\":\n",
        "            # Para arrays, pegamos o primeiro elemento (poderia ser expandido)\n",
        "            df = df.withColumn(field, F.col(\"VL_LIST_DATA.contatos\")[0].getItem(parts[1]))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Aplicar a transformação\n",
        "df_expanded = extract_dynamic_fields(df_final)\n",
        "\n",
        "# Mostrar schema e dados expandidos\n",
        "print(\"Schema expandido:\")\n",
        "df_expanded.printSchema()\n",
        "\n",
        "print(\"Dados expandidos:\")\n",
        "df_expanded.show(truncate=False)\n",
        "\n",
        "# Opcional: Criar versão normalizada (uma linha por contato)\n",
        "df_contatos = df_final.select(\n",
        "    \"*\",\n",
        "    F.explode(\"VL_LIST_DATA.contatos\").alias(\"contato\")\n",
        ").select(\n",
        "    \"*\",\n",
        "    F.col(\"contato.tipo\").alias(\"contato_tipo\"),\n",
        "    F.col(\"contato.valor\").alias(\"contato_valor\")\n",
        ").drop(\"VL_LIST_DATA\", \"contato\")\n",
        "\n",
        "print(\"Dados normalizados (um registro por contato):\")\n",
        "df_contatos.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-Dx-LGOqAhX",
        "outputId": "9dced4b6-463c-4b2c-dc72-b0ee65591f68"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema expandido:\n",
            "root\n",
            " |-- row_id: long (nullable = false)\n",
            " |-- pessoa: struct (nullable = false)\n",
            " |    |-- ativo: boolean (nullable = true)\n",
            " |    |-- idade: integer (nullable = true)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |    |-- salario: float (nullable = true)\n",
            " |-- empresa: struct (nullable = false)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |-- VL_LIST_DATA: struct (nullable = true)\n",
            " |    |-- detalhes: struct (nullable = true)\n",
            " |    |    |-- cargo: string (nullable = true)\n",
            " |    |    |-- setor: string (nullable = true)\n",
            " |    |-- contatos: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- tipo: string (nullable = true)\n",
            " |    |    |    |-- valor: string (nullable = true)\n",
            " |-- contatos_tipo: string (nullable = true)\n",
            " |-- contatos_valor: string (nullable = true)\n",
            " |-- detalhes_cargo: string (nullable = true)\n",
            " |-- detalhes_setor: string (nullable = true)\n",
            "\n",
            "Dados expandidos:\n",
            "+----------+--------------------------+-----------+-------------------------------------------+-------------+--------------+--------------+--------------+\n",
            "|row_id    |pessoa                    |empresa    |VL_LIST_DATA                               |contatos_tipo|contatos_valor|detalhes_cargo|detalhes_setor|\n",
            "+----------+--------------------------+-----------+-------------------------------------------+-------------+--------------+--------------+--------------+\n",
            "|0         |{true, 30, João, 1500.75} |{ACME Ltda}|{{Analista, TI}, [{email, joao@email.com}]}|email        |joao@email.com|Analista      |TI            |\n",
            "|8589934592|{false, 25, Maria, 2000.0}|{Beta Corp}|{{Gerente, RH}, [{telefone, 1199999999}]}  |telefone     |1199999999    |Gerente       |RH            |\n",
            "+----------+--------------------------+-----------+-------------------------------------------+-------------+--------------+--------------+--------------+\n",
            "\n",
            "Dados normalizados (um registro por contato):\n",
            "+----------+--------------------------+-----------+------------+--------------+\n",
            "|row_id    |pessoa                    |empresa    |contato_tipo|contato_valor |\n",
            "+----------+--------------------------+-----------+------------+--------------+\n",
            "|0         |{true, 30, João, 1500.75} |{ACME Ltda}|email       |joao@email.com|\n",
            "|8589934592|{false, 25, Maria, 2000.0}|{Beta Corp}|telefone    |1199999999    |\n",
            "+----------+--------------------------+-----------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Versão Três"
      ],
      "metadata": {
        "id": "5WMDcEOJq6Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def create_initial_dataframe(raw_objects, spark):\n",
        "    \"\"\"Cria o DataFrame inicial a partir de objetos raw\"\"\"\n",
        "    data = [(json.dumps(obj),) for obj in raw_objects]\n",
        "    return spark.createDataFrame(data, [\"linha\"])\n",
        "\n",
        "def parse_json_data(df):\n",
        "    \"\"\"Parseia o JSON contido na coluna 'linha'\"\"\"\n",
        "    json_schema = StructType([\n",
        "        StructField(\"metadata\", ArrayType(StringType())),\n",
        "        StructField(\"valores\", ArrayType(StringType()))\n",
        "    ])\n",
        "    return df.withColumn(\"parsed\", F.from_json(\"linha\", json_schema)) \\\n",
        "             .select(\"parsed.*\") \\\n",
        "             .withColumn(\"row_id\", F.monotonically_increasing_id())\n",
        "\n",
        "def explode_key_value_pairs(df):\n",
        "    \"\"\"Transforma os pares metadata-valores em linhas separadas\"\"\"\n",
        "    return df.withColumn(\"zipped\", F.arrays_zip(\"metadata\", \"valores\")) \\\n",
        "             .withColumn(\"pair\", F.explode(\"zipped\")) \\\n",
        "             .select(\n",
        "                 \"row_id\",\n",
        "                 F.col(\"pair.metadata\").alias(\"chave\"),\n",
        "                 F.col(\"pair.valores\").alias(\"valor\")\n",
        "             )\n",
        "\n",
        "def separate_special_columns(df, special_columns):\n",
        "    \"\"\"Separa colunas especiais do restante dos dados\"\"\"\n",
        "    condition = F.col(\"chave\").isin(special_columns)\n",
        "    df_special = df.filter(condition)\n",
        "    df_main = df.filter(~condition)\n",
        "    return df_main, df_special\n",
        "\n",
        "def pivot_main_data(df, replace_char='.', replacement='_'):\n",
        "    \"\"\"Pivota os dados principais substituindo caracteres nos nomes das colunas\"\"\"\n",
        "    return df.withColumn(\"chave\", F.regexp_replace(\"chave\", re.escape(replace_char), replacement)) \\\n",
        "             .groupBy(\"row_id\").pivot(\"chave\").agg(F.first(\"valor\"))\n",
        "\n",
        "def parse_special_data(value, structure):\n",
        "    \"\"\"Função genérica para parsear dados especiais\"\"\"\n",
        "    result = defaultdict(dict)\n",
        "    items = value.split(';')\n",
        "    for item in items:\n",
        "        if '=' in item:\n",
        "            key, val = item.split('=', 1)\n",
        "            parts = key.split('.')\n",
        "            current = result\n",
        "            for part in parts[:-1]:\n",
        "                current = current.setdefault(part, {})\n",
        "            current[parts[-1]] = val\n",
        "    return json.dumps(result)\n",
        "\n",
        "def process_special_columns(df, column_name, structure):\n",
        "    \"\"\"Processa colunas especiais de acordo com a estrutura definida\"\"\"\n",
        "    parse_udf = F.udf(lambda x: parse_special_data(x, structure), StringType())\n",
        "    return df.withColumn(\"parsed_data\", parse_udf(\"valor\")) \\\n",
        "             .select(\"row_id\", \"parsed_data\")\n",
        "\n",
        "def infer_and_cast_types(df, exclude_columns=[]):\n",
        "    \"\"\"Infere e converte tipos de colunas automaticamente\"\"\"\n",
        "    def infer_type(value):\n",
        "        if isinstance(value, str):\n",
        "            if value.lower() in (\"true\", \"false\"):\n",
        "                return BooleanType()\n",
        "            elif re.match(r\"^-?\\d+$\", value):\n",
        "                return IntegerType()\n",
        "            elif re.match(r\"^-?\\d+\\.\\d+$\", value):\n",
        "                return FloatType()\n",
        "        return StringType()\n",
        "\n",
        "    for col_name in [c for c in df.columns if c not in exclude_columns]:\n",
        "        sample = df.select(col_name).filter(F.col(col_name).isNotNull()).first()\n",
        "        if sample:\n",
        "            dtype = infer_type(sample[0])\n",
        "            df = df.withColumn(col_name, F.col(col_name).cast(dtype))\n",
        "    return df\n",
        "\n",
        "def create_nested_structures(df, prefixes):\n",
        "    \"\"\"Cria estruturas aninhadas para os prefixos especificados\"\"\"\n",
        "    for prefix in prefixes:\n",
        "        cols = [c for c in df.columns if c.startswith(f\"{prefix}_\")]\n",
        "        if cols:\n",
        "            fields = [c[len(prefix)+1:] for c in cols]\n",
        "            struct_cols = [F.col(c).alias(f) for c, f in zip(cols, fields)]\n",
        "            df = df.withColumn(prefix, F.struct(*struct_cols)).drop(*cols)\n",
        "    return df\n",
        "\n",
        "def process_list_data(df, column_name, schema):\n",
        "    \"\"\"Processa colunas com dados em lista de acordo com o schema fornecido\"\"\"\n",
        "    return df.withColumn(column_name, F.from_json(column_name, schema))\n",
        "\n",
        "def transform_data(raw_objects, spark, config):\n",
        "    \"\"\"\n",
        "    Função principal que orquestra todo o processamento\n",
        "\n",
        "    Args:\n",
        "        raw_objects: Lista de objetos com os dados brutos\n",
        "        spark: Sessão Spark\n",
        "        config: Dicionário com configurações:\n",
        "            {\n",
        "                \"special_columns\": [\"VL_LIST_DATA\"],  # Colunas especiais para tratamento diferenciado\n",
        "                \"structure_definitions\": {            # Definições de estrutura para colunas especiais\n",
        "                    \"VL_LIST_DATA\": {\n",
        "                        \"detalhes\": [\"cargo\", \"setor\"],\n",
        "                        \"contatos\": [\"tipo\", \"valor\"]\n",
        "                    }\n",
        "                },\n",
        "                \"nested_prefixes\": [\"pessoa\", \"empresa\"]  # Prefixos para criar estruturas aninhadas\n",
        "            }\n",
        "    \"\"\"\n",
        "    # 1. Criação do DataFrame inicial\n",
        "    df = create_initial_dataframe(raw_objects, spark)\n",
        "\n",
        "    # 2. Parse do JSON\n",
        "    df = parse_json_data(df)\n",
        "\n",
        "    # 3. Explodir pares chave-valor\n",
        "    df = explode_key_value_pairs(df)\n",
        "\n",
        "    # 4. Separar colunas especiais\n",
        "    df_main, df_special = separate_special_columns(df, config.get(\"special_columns\", []))\n",
        "\n",
        "    # 5. Pivotar dados principais\n",
        "    df_pivoted = pivot_main_data(df_main)\n",
        "\n",
        "    # 6. Processar colunas especiais\n",
        "    processed_special = []\n",
        "    for col_name in config.get(\"special_columns\", []):\n",
        "        if col_name in [row.chave for row in df_special.select(\"chave\").distinct().collect()]:\n",
        "            structure = config[\"structure_definitions\"].get(col_name, {})\n",
        "            df_col = df_special.filter(F.col(\"chave\") == col_name)\n",
        "            processed = process_special_columns(df_col, col_name, structure)\n",
        "            processed_special.append(processed)\n",
        "\n",
        "    # 7. Juntar todos os dados\n",
        "    df_combined = df_pivoted\n",
        "    for special_df in processed_special:\n",
        "        df_combined = df_combined.join(special_df, \"row_id\")\n",
        "\n",
        "    # 8. Inferir tipos de dados\n",
        "    exclude = [\"row_id\"] + [f\"parsed_{col}\" for col in config.get(\"special_columns\", [])]\n",
        "    df_combined = infer_and_cast_types(df_combined, exclude)\n",
        "\n",
        "    # 9. Criar estruturas aninhadas\n",
        "    df_final = create_nested_structures(df_combined, config.get(\"nested_prefixes\", []))\n",
        "\n",
        "    # 10. Processar dados especiais com schema\n",
        "    for col_name in config.get(\"special_columns\", []):\n",
        "        if f\"parsed_{col_name}\" in df_final.columns:\n",
        "            schema_def = config[\"structure_definitions\"].get(col_name, {})\n",
        "            schema = build_schema_from_definition(schema_def)\n",
        "            df_final = process_list_data(df_final, col_name, schema)\n",
        "            df_final = df_final.drop(f\"parsed_{col_name}\")\n",
        "\n",
        "    return df_final\n",
        "\n",
        "def build_schema_from_definition(definition):\n",
        "    \"\"\"Constrói um schema Spark a partir da definição de estrutura\"\"\"\n",
        "    fields = []\n",
        "\n",
        "    if \"detalhes\" in definition:\n",
        "        detail_fields = [StructField(field, StringType()) for field in definition[\"detalhes\"]]\n",
        "        fields.append(StructField(\"detalhes\", StructType(detail_fields)))\n",
        "\n",
        "    if \"contatos\" in definition:\n",
        "        contact_fields = [StructField(field, StringType()) for field in definition[\"contatos\"]]\n",
        "        fields.append(StructField(\"contatos\", ArrayType(StructType(contact_fields))))\n",
        "\n",
        "    return StructType(fields)\n",
        "\n",
        "# Exemplo de uso:\n",
        "config = {\n",
        "    \"special_columns\": [\"VL_LIST_DATA\"],\n",
        "    \"structure_definitions\": {\n",
        "        \"VL_LIST_DATA\": {\n",
        "            \"detalhes\": [\"cargo\", \"setor\"],\n",
        "            \"contatos\": [\"tipo\", \"valor\"]\n",
        "        }\n",
        "    },\n",
        "    \"nested_prefixes\": [\"pessoa\", \"empresa\"]\n",
        "}\n",
        "\n",
        "df_result = transform_data(raw_objects, spark, config)\n",
        "df_result.printSchema()\n",
        "df_result.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPgjE93SqTfg",
        "outputId": "a78f0113-ece9-4b42-a669-44c28b5f7edd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- row_id: long (nullable = false)\n",
            " |-- parsed_data: string (nullable = true)\n",
            " |-- pessoa: struct (nullable = false)\n",
            " |    |-- ativo: boolean (nullable = true)\n",
            " |    |-- idade: integer (nullable = true)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |    |-- salario: float (nullable = true)\n",
            " |-- empresa: struct (nullable = false)\n",
            " |    |-- nome: string (nullable = true)\n",
            "\n",
            "+----------+-------------------------------------------------------------------------------------------------------------------+--------------------------+-----------+\n",
            "|row_id    |parsed_data                                                                                                        |pessoa                    |empresa    |\n",
            "+----------+-------------------------------------------------------------------------------------------------------------------+--------------------------+-----------+\n",
            "|0         |{\"detalhes\": {\"cargo\": \"Analista\", \"setor\": \"TI\"}, \"contatos\": {\"0\": {\"tipo\": \"email\", \"valor\": \"joao@email.com\"}}}|{true, 30, João, 1500.75} |{ACME Ltda}|\n",
            "|8589934592|{\"detalhes\": {\"cargo\": \"Gerente\", \"setor\": \"RH\"}, \"contatos\": {\"0\": {\"tipo\": \"telefone\", \"valor\": \"1199999999\"}}}  |{false, 25, Maria, 2000.0}|{Beta Corp}|\n",
            "+----------+-------------------------------------------------------------------------------------------------------------------+--------------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "\n",
        "def safe_extract_nested_data(df, column_name):\n",
        "    \"\"\"\n",
        "    Extrai dados aninhados de forma segura, verificando se a coluna existe\n",
        "    e se tem a estrutura esperada.\n",
        "    \"\"\"\n",
        "    # Verifica se a coluna existe no DataFrame\n",
        "    if column_name not in df.columns:\n",
        "        raise ValueError(f\"A coluna '{column_name}' não existe no DataFrame. Colunas disponíveis: {df.columns}\")\n",
        "\n",
        "    # Obtém o schema da coluna\n",
        "    col_schema = [f for f in df.schema.fields if f.name == column_name][0].dataType\n",
        "\n",
        "    # Verifica se é uma struct\n",
        "    if not isinstance(col_schema, StructType):\n",
        "        raise ValueError(f\"A coluna '{column_name}' não é uma estrutura (StructType). Tipo encontrado: {type(col_schema)}\")\n",
        "\n",
        "    # Extrai campos dinamicamente\n",
        "    fields_to_extract = []\n",
        "    for field in col_schema.fields:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            for subfield in field.dataType.fields:\n",
        "                fields_to_extract.append((f\"{field.name}_{subfield.name}\", f\"{column_name}.{field.name}.{subfield.name}\"))\n",
        "        else:\n",
        "            fields_to_extract.append((f\"{column_name}_{field.name}\", f\"{column_name}.{field.name}\"))\n",
        "\n",
        "    # Adiciona as colunas extraídas\n",
        "    for new_col, col_expr in fields_to_extract:\n",
        "        df = df.withColumn(new_col, F.col(col_expr))\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_dynamic_data(df):\n",
        "    \"\"\"\n",
        "    Processa o DataFrame dinamicamente, verificando e tratando colunas aninhadas.\n",
        "    \"\"\"\n",
        "    # Verifica colunas que podem conter dados aninhados\n",
        "    nested_columns = [f.name for f in df.schema.fields\n",
        "                     if isinstance(f.dataType, StructType) or\n",
        "                        (isinstance(f.dataType, ArrayType) and isinstance(f.dataType.elementType, StructType))]\n",
        "\n",
        "    if not nested_columns:\n",
        "        print(\"Nenhuma coluna aninhada encontrada.\")\n",
        "        return df\n",
        "\n",
        "    print(f\"Colunas aninhadas encontradas: {nested_columns}\")\n",
        "\n",
        "    # Processa cada coluna aninhada\n",
        "    for col_name in nested_columns:\n",
        "        try:\n",
        "            df = safe_extract_nested_data(df, col_name)\n",
        "            print(f\"Coluna '{col_name}' processada com sucesso.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar coluna '{col_name}': {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return df\n",
        "\n",
        "# Exemplo de uso:\n",
        "try:\n",
        "    # Processa o DataFrame dinamicamente\n",
        "    df_processed = process_dynamic_data(df_final)\n",
        "\n",
        "    # Mostra os resultados\n",
        "    print(\"\\nSchema após processamento:\")\n",
        "    df_processed.printSchema()\n",
        "\n",
        "    print(\"\\nDados após processamento:\")\n",
        "    df_processed.show(truncate=False)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erro durante o processamento: {str(e)}\")\n",
        "    # Mostra o schema atual para diagnóstico\n",
        "    print(\"\\nSchema atual do DataFrame:\")\n",
        "    df_final.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yacFcHyrxtl",
        "outputId": "6f28b215-7e49-47b3-9aaa-ba208aeb4818"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colunas aninhadas encontradas: ['pessoa', 'empresa', 'VL_LIST_DATA']\n",
            "Coluna 'pessoa' processada com sucesso.\n",
            "Coluna 'empresa' processada com sucesso.\n",
            "Coluna 'VL_LIST_DATA' processada com sucesso.\n",
            "\n",
            "Schema após processamento:\n",
            "root\n",
            " |-- row_id: long (nullable = false)\n",
            " |-- pessoa: struct (nullable = false)\n",
            " |    |-- ativo: boolean (nullable = true)\n",
            " |    |-- idade: integer (nullable = true)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |    |-- salario: float (nullable = true)\n",
            " |-- empresa: struct (nullable = false)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |-- VL_LIST_DATA: struct (nullable = true)\n",
            " |    |-- detalhes: struct (nullable = true)\n",
            " |    |    |-- cargo: string (nullable = true)\n",
            " |    |    |-- setor: string (nullable = true)\n",
            " |    |-- contatos: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- tipo: string (nullable = true)\n",
            " |    |    |    |-- valor: string (nullable = true)\n",
            " |-- pessoa_ativo: boolean (nullable = true)\n",
            " |-- pessoa_idade: integer (nullable = true)\n",
            " |-- pessoa_nome: string (nullable = true)\n",
            " |-- pessoa_salario: float (nullable = true)\n",
            " |-- empresa_nome: string (nullable = true)\n",
            " |-- detalhes_cargo: string (nullable = true)\n",
            " |-- detalhes_setor: string (nullable = true)\n",
            " |-- VL_LIST_DATA_contatos: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- tipo: string (nullable = true)\n",
            " |    |    |-- valor: string (nullable = true)\n",
            "\n",
            "\n",
            "Dados após processamento:\n",
            "+----------+--------------------------+-----------+-------------------------------------------+------------+------------+-----------+--------------+------------+--------------+--------------+-------------------------+\n",
            "|row_id    |pessoa                    |empresa    |VL_LIST_DATA                               |pessoa_ativo|pessoa_idade|pessoa_nome|pessoa_salario|empresa_nome|detalhes_cargo|detalhes_setor|VL_LIST_DATA_contatos    |\n",
            "+----------+--------------------------+-----------+-------------------------------------------+------------+------------+-----------+--------------+------------+--------------+--------------+-------------------------+\n",
            "|0         |{true, 30, João, 1500.75} |{ACME Ltda}|{{Analista, TI}, [{email, joao@email.com}]}|true        |30          |João       |1500.75       |ACME Ltda   |Analista      |TI            |[{email, joao@email.com}]|\n",
            "|8589934592|{false, 25, Maria, 2000.0}|{Beta Corp}|{{Gerente, RH}, [{telefone, 1199999999}]}  |false       |25          |Maria      |2000.0        |Beta Corp   |Gerente       |RH            |[{telefone, 1199999999}] |\n",
            "+----------+--------------------------+-----------+-------------------------------------------+------------+------------+-----------+--------------+------------+--------------+--------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalização"
      ],
      "metadata": {
        "id": "5ks_wLv7tTuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "\n",
        "# Iniciar sessão Spark\n",
        "spark = SparkSession.builder.appName(\"NestedData\").getOrCreate()\n",
        "\n",
        "# 1. Dados de exemplo CORRETOS\n",
        "data = [\n",
        "    (1, '{\"nome\": \"João\", \"idade\": 30}', [{\"produto\": \"A\", \"preco\": 10.5}], {\"chave\": \"valor\"}),\n",
        "    (2, '{\"nome\": \"Maria\", \"idade\": 25}', [{\"produto\": \"B\", \"preco\": 20.0}], {\"chave\": \"valor2\"}),\n",
        "    (3, '{\"nome\": \"Carlos\"}', None, None)\n",
        "]\n",
        "\n",
        "# 2. Schema CORRETO\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"json_col\", StringType()),\n",
        "    StructField(\"array_col\", ArrayType(StructType([\n",
        "        StructField(\"produto\", StringType()),\n",
        "        StructField(\"preco\", DoubleType())\n",
        "    ]))),\n",
        "    StructField(\"map_col\", MapType(StringType(), StringType()))\n",
        "])\n",
        "\n",
        "# 3. Criar DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# 4. Função CORRETA para processar JSON\n",
        "def parse_json_safely(df, json_column, output_column):\n",
        "    \"\"\"Parseia uma coluna JSON de forma segura\"\"\"\n",
        "    json_schema = StructType([\n",
        "        StructField(\"nome\", StringType()),\n",
        "        StructField(\"idade\", IntegerType())\n",
        "    ])\n",
        "\n",
        "    return df.withColumn(output_column, F.from_json(F.col(json_column), json_schema))\n",
        "\n",
        "# 5. Processamento PASSO-A-PASSO\n",
        "df = parse_json_safely(df, \"json_col\", \"json_parsed\")\n",
        "\n",
        "# Extrair campos do JSON\n",
        "df = df.withColumn(\"nome\", F.col(\"json_parsed.nome\"))\n",
        "df = df.withColumn(\"idade\", F.col(\"json_parsed.idade\"))\n",
        "\n",
        "# Processar array de structs\n",
        "df = df.withColumn(\"primeiro_produto\", F.col(\"array_col\").getItem(0).getField(\"produto\"))\n",
        "df = df.withColumn(\"primeiro_preco\", F.col(\"array_col\").getItem(0).getField(\"preco\"))\n",
        "\n",
        "# Processar map\n",
        "df = df.withColumn(\"valor_chave\", F.col(\"map_col\").getItem(\"chave\"))\n",
        "\n",
        "# 6. Mostrar resultados CORRETOS\n",
        "print(\"Schema final:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\nDados processados:\")\n",
        "df.select(\n",
        "    \"id\",\n",
        "    \"array_col\",\n",
        "    \"map_col\",\n",
        "    \"nome\",\n",
        "    \"idade\",\n",
        "    \"primeiro_produto\",\n",
        "    \"primeiro_preco\",\n",
        "    \"valor_chave\"\n",
        ").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNFm13yJsJ7A",
        "outputId": "e0367e04-b471-49a0-fd08-71b68914b821"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema final:\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- json_col: string (nullable = true)\n",
            " |-- array_col: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- produto: string (nullable = true)\n",
            " |    |    |-- preco: double (nullable = true)\n",
            " |-- map_col: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- json_parsed: struct (nullable = true)\n",
            " |    |-- nome: string (nullable = true)\n",
            " |    |-- idade: integer (nullable = true)\n",
            " |-- nome: string (nullable = true)\n",
            " |-- idade: integer (nullable = true)\n",
            " |-- primeiro_produto: string (nullable = true)\n",
            " |-- primeiro_preco: double (nullable = true)\n",
            " |-- valor_chave: string (nullable = true)\n",
            "\n",
            "\n",
            "Dados processados:\n",
            "+---+-----------+-----------------+------+-----+----------------+--------------+-----------+\n",
            "|id |array_col  |map_col          |nome  |idade|primeiro_produto|primeiro_preco|valor_chave|\n",
            "+---+-----------+-----------------+------+-----+----------------+--------------+-----------+\n",
            "|1  |[{A, 10.5}]|{chave -> valor} |João  |30   |A               |10.5          |valor      |\n",
            "|2  |[{B, 20.0}]|{chave -> valor2}|Maria |25   |B               |20.0          |valor2     |\n",
            "|3  |NULL       |NULL             |Carlos|NULL |NULL            |NULL          |NULL       |\n",
            "+---+-----------+-----------------+------+-----+----------------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_json_safely(df, json_column, output_column):\n",
        "    \"\"\"Versão com tratamento de erros completo\"\"\"\n",
        "    json_schema = StructType([\n",
        "        StructField(\"nome\", StringType()),\n",
        "        StructField(\"idade\", IntegerType())\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        # Tentar parsear o JSON\n",
        "        df = df.withColumn(output_column, F.from_json(F.col(json_column), json_schema))\n",
        "\n",
        "        # Extrair campos com fallback para nulo\n",
        "        df = df.withColumn(\"nome\", F.coalesce(F.col(f\"{output_column}.nome\"), F.lit(\"\")))\n",
        "        df = df.withColumn(\"idade\", F.coalesce(F.col(f\"{output_column}.idade\"), F.lit(None).cast(IntegerType())))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao processar JSON: {str(e)}\")\n",
        "        df = df.withColumn(\"nome\", F.lit(None).cast(StringType()))\n",
        "        df = df.withColumn(\"idade\", F.lit(None).cast(IntegerType()))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "lSoip23muAGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}